{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a314ddb-2f6b-492a-8c9d-faa1bf741a60",
   "metadata": {},
   "source": [
    "# Технологии искусственного интеллекта\n",
    "\n",
    "© Петров М.В., старший преподаватель кафедры суперкомпьютеров и общей информатики, Самарский университет\n",
    "\n",
    "## Лекция 5. Линейная регрессия\n",
    "\n",
    "### Содержание\n",
    "\n",
    "1. [Введение](#5.1-Введение)\n",
    "2. [Метод наименьших квадратов ($\\textit{Ordinary Least Squares}$, $OLS$) МНК](#5.2-Метод-наименьших-квадратов-($\\textit{Ordinary-Least-Squares}$,-$OLS$)-МНК)\n",
    "3. [$Ridge$ регрессия](#5.3-$\\textit{Ridge}$-регрессия)\n",
    "4. [$LASSO$ регрессия](#5.4-$LASSO$-регрессия)\n",
    "5. [$\\textit{Elastic Net}$ регрессия](#5.5-$\\textit{Elastic-Net}$-регрессия)\n",
    "6. [Метрики качества линейных регрессионных моделей](#5.6-Метрики-качества-линейных-регрессионных-моделей)\n",
    "7. [Пример](#5.7-Пример)\n",
    "8. [Датасет `Rain in Australia`](#5.8-Датасет-Rain-in-Australia)\n",
    "9. [Подготовка данных](#5.9-Подготовка-данных)\n",
    "10. [Линейная регрессия для `Rain in Australia`](#5.10-Линейная-регрессия-для-Rain-in-Australia)\n",
    "11. [Recursive Feature Elimination (RFE)](#5.11-Recursive-Feature-Elimination-(RFE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e44a74-b543-4577-93c0-4f76dcb47804",
   "metadata": {},
   "source": [
    "### 5.1 Введение\n",
    "\n",
    "Источники:\n",
    " - [Открытый курс машинного обучения. Тема 4. Линейные модели классификации и регрессии @ Хабр](https://habr.com/ru/companies/ods/articles/323890/)\n",
    " - [Знакомьтесь, линейные модели @ Хабр](https://habr.com/ru/articles/278513/)\n",
    " - [Линейные модели @ scikit-learn.ru](https://scikit-learn.ru/1-1-linear-models/)\n",
    " - [Linear Models @ scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "\n",
    "Пусть дан набор из $M$ пар исходных данных $(X_i, Y_i)$, где  \n",
    "$\n",
    "\\quad \\begin{matrix*}[l]\n",
    "X_i \\in \\mathbb{R}^D \\\\\n",
    "Y_i \\in \\mathbb{R}^K \\\\\n",
    "M,D,K \\in \\mathbb{N} \\\\\n",
    "i = \\overline{1, M} \\\\\n",
    "\\mathbb{R} \\textit{ - множество вещественных чисел} \\\\\n",
    "\\mathbb{N} \\textit{ - множество натуральных чисел}\n",
    "\\end{matrix*}\n",
    "$  \n",
    "$X_i$ &ndash; это последовательность вещественных чисел длиной $D$ &ndash; независимые переменные (также называются *факторами* или *регрессорами*), $Y_i$ &ndash; это последовательность вещественных чисел длиной $K$ &ndash; зависимые переменные (также называются *объясняемыми переменными*).  \n",
    "В самом простом случае $D = K = 1$, то есть нам заданы пары чисел, например, (рост человека, вес человека), или (объем детали, вес детали), или (время ожидания в очереди, признак ухода клиента).  \n",
    "Чаще всего $D > 1$, а $K = 1$, то есть для каждого $Y$ даны несколько $X$, например, ((рост, возраст), вес) или ((продажи вчера, продажи позавчера, продажи позапозавчера), продажи сегодня).  \n",
    "Мы вдруг делаем неожиданное предположение: наши $Y$ не просто зависят, а *линейно* зависят от $X$, то есть $$Y = W^TX,$$ где $W$ &ndash; это параметры модели в виде вещественной матрицы размерности $D \\times K$.  \n",
    "\n",
    "Однако сразу же вспомним про два важных аспекта. Во-первых, нет никаких причин, чтобы $W^TX$ вот так вот строго равнялось $Y$. Ведь и исходные данные мы могли измерить (и скорее всего измерили) с погрешностью. Кроме того, возможно (и даже скорее всего почти точно) мы упустили из виду какие-то факторы $X$, которые тоже влияют на $Y$. Поэтому в модели надо учесть случайную ошибку. \n",
    "Во-вторых, нет никакой гарантии, что $Y$ линейно зависят сразу напрямую от $X$. Ведь возможно они зависят от чего-то другого, что в свою очередь зависит от $X$. Например, $Y$ может быть равен $W^T \\cdot X^2$ или $W^T \\cdot \\log X$. А тогда почему, собственно, мы жаждем именно зависимости самого $Y$, а не какого-то значения, зависящего от $Y$, допустим, $\\log Y = W^TX$.  \n",
    "\n",
    "Таким образом мы приходим к обобщенной постановке линейной задачи:\n",
    "$$\n",
    "\\begin{matrix*}[l]\n",
    "Y^* = W^TX^* + \\Xi, \\\\\n",
    "X^* = f(X), \\\\\n",
    "Y^* = g(Y), \\\\\n",
    "\\Xi \\textit{ - случайная величина}.\n",
    "\\end{matrix*}\n",
    "$$  \n",
    "\n",
    "Несмотря на то, что $f$ и $g$ могут быть нелинейными функциями, и $Y$ в результате может весьма нелинейно зависеть от $X$, модель все равно остается линейной относительно параметров $W$. Именно поэтому она и называется линейной моделью.  \n",
    "Из-за случайностей мы не можем легко рассчитать искомые коэффициенты $W$ нашей модели. Поэтому придется решить оптимизационную задачу вида \n",
    "$$\\hat{W} = argmin \\; F(W \\mid X^*,Y^*),$$\n",
    "где $F$ &ndash; некий функционал.  \n",
    "\n",
    "> При $K = 1$:\n",
    "> $$\n",
    "\\begin{matrix*}[l]\n",
    "y^* = w^TX^* + \\xi, \\\\\n",
    "X^* = f(X), \\\\\n",
    "y^* = g(y), \\\\\n",
    "\\xi \\textit{ - случайная величина}. \\\\\n",
    "\\\\\n",
    "\\hat{w} = argmin \\; F(w \\mid X^*,y^*).\n",
    "\\end{matrix*}\n",
    "$$\n",
    "\n",
    "Пусть далее для простоты $K = 1$. Можем выписать выражение для каждого конкретного наблюдения:\n",
    "\n",
    "$$y_i = \\displaystyle \\sum_{j = 0}^{D} w_j \\cdot X_{ij} + \\xi_i, \\quad i = \\overline{1, M}.$$\n",
    "\n",
    "Также на модель накладываются следующие ограничения (иначе это будет какая то другая регрессия, но точно не линейная):\n",
    "- матожидание случайных ошибок равно нулю: $\\forall i : \\mathbb{E}[\\xi_i] = 0$;\n",
    "- дисперсия случайных ошибок одинакова и конечна: $\\forall i : D[\\xi_i] = \\sigma^2 < \\infty$;\n",
    "- случайные ошибки не скоррелированы: $\\forall i \\neg j : cov(\\xi_i, \\xi_j) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d3b99-a375-4933-9dbb-7ea4e9d81940",
   "metadata": {},
   "source": [
    "### 5.2 Метод наименьших квадратов ($\\textit{Ordinary Least Squares}$, $OLS$) МНК\n",
    "\n",
    "Один из способов вычислить значения параметров модели является метод наименьших квадратов (МНК), который минимизирует среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным моделью:\n",
    "$$\\min_w \\|Xw-y\\|_2^2.$$\n",
    "\n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d453f9-5f3e-4abe-91e5-73e03008c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Генерируем данные для X и y\n",
    "X, y = make_regression(n_samples=20, noise=10, n_features=1, random_state=0)\n",
    "\n",
    "# Разделение данных на train и test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=3)\n",
    "\n",
    "# Создание объекта класса линейной регрессии\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Обучение\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "y_pred = regr.predict(test_X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(train_X, train_y, color=\"blue\")\n",
    "plt.scatter(test_X, test_y, color=\"red\")\n",
    "plt.plot(test_X, y_pred, color=\"green\")\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3c0cf-8b8a-46f2-87d0-8c899f564610",
   "metadata": {},
   "source": [
    "### 5.3 $\\textit{Ridge}$ регрессия\n",
    "\n",
    "Гребневая регрессия или ридж-регрессия (англ. $\\textit{Ridge Regression}$) &ndash; один из методов понижения размерности. Применяется для борьбы с избыточностью данных, когда независимые переменные коррелируют друг с другом, вследствие чего проявляется неустойчивость оценок коэффициентов многомерной линейной регрессии. Мультиколлинеарность (англ. multicollinearity) &ndash; наличие линейной зависимости между независимыми переменными регрессионной модели. Различают полную коллинеарность и частичную или просто мультиколлинеарность &ndash; наличие сильной корреляции между независимыми переменными. Получаемое решение минимизирует следующий функционал:\n",
    "\n",
    "$$\\min_w \\|Xw-y\\|_2^2 + \\alpha \\|w\\|_2^2,$$\n",
    "\n",
    "где $\\alpha > 0$ &ndash; параметр регуляризации.\n",
    "\n",
    "Пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc98501-09ad-4a5e-b595-74f1fdd2c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "labels = []\n",
    "\n",
    "for a in np.arange(0.2, 4, 0.8):\n",
    "    # Создание объекта класса Ridge регрессии\n",
    "    ridge_regression = linear_model.Ridge(alpha=a)  # alpha - величина регуляризации\n",
    "    # Обучение\n",
    "    ridge_regression.fit(train_X, train_y)\n",
    "    # Предсказание результата\n",
    "    y_pred.append(ridge_regression.predict(test_X))\n",
    "    # Вывод коэффициента детерминации\n",
    "    print(f\"Alpha: {a:.1f}, score: {ridge_regression.score(test_X, test_y)}\")\n",
    "    labels.append(f'alpha = {a:.1f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(train_X, train_y, color=\"blue\")\n",
    "plt.scatter(test_X, test_y, color=\"red\")\n",
    "for y in y_pred:\n",
    "    plt.plot(test_X, y, color=np.random.rand(3,), linewidth=3)\n",
    "\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee018d-ef39-4a95-a01c-c5e3312f7c11",
   "metadata": {},
   "source": [
    "### 5.4 $LASSO$ регрессия\n",
    "\n",
    "Метод регрессии лассо (англ. $LASSO$, *Least Absolute Shrinkage and Selection Operator*) похож на гребневую регрессию, но он использует другое ограничение на коэффициенты:\n",
    "\n",
    "$$\\min_w \\frac{1}{2M} \\|Xw-y\\|_2^2 + \\alpha \\|w\\|_1,$$\n",
    "\n",
    "где $\\alpha > 0$ &ndash; параметр регуляризации.\n",
    "\n",
    "Пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e528c85-2bc3-4e73-98a4-b782e555dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "labels = []\n",
    "\n",
    "for a in np.arange(0.2, 4, 0.8):\n",
    "    # Создание объекта класса LASSO регрессии\n",
    "    lasso_regression = linear_model.Lasso(alpha=a)  # alpha — величина регуляризации\n",
    "    # Обучение\n",
    "    lasso_regression.fit(train_X, train_y)\n",
    "    # Предсказание результата\n",
    "    y_pred.append(lasso_regression.predict(test_X))\n",
    "    # Вывод коэффициента детерминации\n",
    "    print(f\"Alpha: {a:.1f}, score: {lasso_regression.score(test_X, test_y)}\")\n",
    "    labels.append(f'alpha = {a:.1f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(train_X, train_y, color=\"blue\")\n",
    "plt.scatter(test_X, test_y, color=\"red\")\n",
    "for y in y_pred:\n",
    "    plt.plot(test_X, y, color=np.random.rand(3,), linewidth=3)\n",
    "\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08045b-9029-4d5e-bc11-79f977c5ce3f",
   "metadata": {},
   "source": [
    "### 5.5 $\\textit{Elastic Net}$ регрессия\n",
    "\n",
    "Эластичная сеть (англ. $\\textit{Elastic Net}$):\n",
    "\n",
    "$$\\min_w \\frac{1}{2M} \\|Xw-y\\|_2^2 + \\alpha \\rho \\|w\\|_1 + \\frac{\\alpha (1 - \\rho)}{2} \\|w\\|_2^2,$$\n",
    "\n",
    "где $\\alpha > 0$ и $0 < \\rho < 1$ &ndash; параметры регуляризации. При $\\rho = 1$ мы получаем $LASSO$.\n",
    "\n",
    "Пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111d478-4419-4cd9-a44b-33df6a2cb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "labels = []\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'alpha': np.arange(0.2, 4, 0.8), 'l1_ratio': [0.25, 0.5, 0.75]}\n",
    "elastic_net = linear_model.ElasticNet()\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, parameters)\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "for a in np.arange(0.2, 4, 0.8):\n",
    "    for r in [0.25, 0.5, 0.75]:\n",
    "        # Создание объекта класса Elastic Net регрессии\n",
    "        elastic_net = linear_model.ElasticNet(alpha=a, l1_ratio=r)\n",
    "        # Обучение\n",
    "        elastic_net.fit(train_X, train_y)\n",
    "        # Предсказание результата\n",
    "        y_pred.append(elastic_net.predict(test_X))\n",
    "        # Вывод коэффициента детерминации\n",
    "        print(f\"Alpha: {a:.1f}, l1_ratio: {r:.2f}, score: {elastic_net.score(test_X, test_y)}\")\n",
    "        labels.append(f'alpha = {a:.1f}, l1_ratio: {r:.2f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(train_X, train_y, color=\"blue\")\n",
    "plt.scatter(test_X, test_y, color=\"red\")\n",
    "for y in y_pred:\n",
    "    plt.plot(test_X, y, color=np.random.rand(3,), linewidth=3)\n",
    "\n",
    "ax.legend(labels)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763a880-faa4-4f5d-8777-9f3b26faf3fc",
   "metadata": {},
   "source": [
    "### 5.6 Метрики качества линейных регрессионных моделей\n",
    "\n",
    "Источники:\n",
    "- [Оценка качества в задачах классификации и регрессии](http://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B0_%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B0_%D0%B2_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0%D1%85_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8_%D0%B8_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8)\n",
    "- [Метрики](https://wiki.loginom.ru/metrics.html)\n",
    "- [Метрики качества линейных регрессионных моделей](https://loginom.ru/blog/quality-metrics)\n",
    "\n",
    "#### $MSE$\n",
    "\n",
    "Средняя квадратичная ошибка (англ. $\\textit{Mean Squared Error}$, $MSE$):\n",
    "\n",
    "$$MSE = \\displaystyle\\frac{1}{M}\\sum_{i = 1}^M (y_i - \\hat{y_i})^2,$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "$MSE$ применяется в ситуациях, когда нам надо подчеркнуть большие ошибки и выбрать модель, которая дает меньше больших ошибок прогноза.\n",
    "\n",
    "#### $MAE$\n",
    "\n",
    "Cредняя абсолютная ошибка (англ. $\\textit{Mean Absolute Error}$, $MAE$):\n",
    "\n",
    "$$MAE = \\displaystyle\\frac{1}{M}\\sum_{i = 1}^M |y_i - \\hat{y_i}|,$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Среднеквадратичный функционал $MSE$ сильнее штрафует за большие отклонения по сравнению со среднеабсолютным $MAE$, и поэтому более чувствителен к выбросам. При использовании любого из этих двух функционалов может быть полезно проанализировать, какие объекты вносят наибольший вклад в общую ошибку &ndash; не исключено, что на этих объектах была допущена ошибка при вычислении признаков или целевой величины.\n",
    "\n",
    "Среднеквадратичная ошибка подходит для сравнения двух моделей или для контроля качества во время обучения, но не позволяет сделать выводов о том, на сколько хорошо данная модель решает задачу. Например, $MSE = 10$ является очень плохим показателем, если целевая переменная принимает значения от $0$ до $1$, и очень хорошим, если целевая переменная лежит в интервале $(10000, 100000)$. В таких ситуациях вместо среднеквадратичной ошибки полезно использовать коэффициент детерминации &ndash; $R^2$.\n",
    "\n",
    "#### $R^2$\n",
    "\n",
    "Коэффициент детерминации $R^2$:\n",
    "\n",
    "$$R^2 = \\displaystyle 1 - \\frac{\\displaystyle\\sum_{i = 1}^M (y_i - \\hat{y_i})^2}{\\displaystyle\\sum_{i = 1}^M (y_i - \\overline{y})^2},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака, $\\overline{y}$ &ndash; среднее значение целевого признака.  \n",
    "\n",
    "Коэффициент детерминации является статистической мерой согласия, с помощью которой можно определить, насколько модель линейной регрессии соответствует данным, на которых она построена. Коэффициент детерминации изменяется в диапазоне от $-\\infty$ до $1$. Если он равен $1$, это соответствует идеальной модели, когда все точки наблюдений лежат точно на линии регрессии, т.е. сумма квадратов их отклонений равна $0$. Если коэффициент детерминации равен $0$, это означает, что связь между переменными регрессионной модели отсутствует, и вместо нее для оценки значения выходной переменной можно использовать простое среднее ее наблюдаемых значений.  \n",
    "На практике, если коэффициент детерминации близок к $1$, это указывает на то, что модель работает очень хорошо (имеет высокую значимость), а если к $0$, то это означает низкую значимость модели, когда входная переменная плохо «объясняет» поведение выходной, т.е. линейная зависимость между ними отсутствует. Очевидно, что такая модель будет иметь низкую эффективность.  \n",
    "Кроме того, бывают случаи, когда коэффициент детерминации принимает отрицательные значения (обычно небольшие). Это случается, когда ошибка модели простого среднего становится меньше ошибки регрессионной модели. Таким образом, добавление в модель с константой некоторой переменной только ухудшает её.\n",
    "\n",
    "#### Скорректированный $R^2$\n",
    "\n",
    "Основной проблемой при использовании коэффициента детерминации является то, что он увеличивается (или, по крайней мере, не уменьшается) при добавлении в модель новых переменных, даже если эти переменные никак не связаны с зависимой переменной.  \n",
    "В связи с этим возникают две проблемы. Первая заключается в том, что не все переменные, добавляемые в модель, могут значимо увеличивать её точность, но при этом всегда увеличивают её сложность. Вторая проблема &ndash; с помощью коэффициента детерминации нельзя сравнивать модели с разным числом переменных. Чтобы преодолеть эти проблемы используют альтернативные показатели, одним из которых является скорректированный коэффициент детерминации англ. ($\\textit{Adjasted coefficient of determination}$).  \n",
    "Скорректированный коэффициент детерминации даёт возможность сравнивать модели с разным числом переменных так, чтобы их число не влияло на статистику $R^2$, и накладывает штраф за дополнительно включённые в модель переменные. Вычисляется по формуле:\n",
    "\n",
    "$$R_{adj}^2 = \\displaystyle 1 - \\frac{\\displaystyle\\sum_{i = 1}^M\\frac{(y_i - \\hat{y_i})^2}{M - D}}{\\displaystyle\\sum_{i = 1}^M\\frac{(y_i - \\overline{y})^2}{M - 1}},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака, $\\overline{y}$ &ndash; среднее значение целевого признака, $M$ &ndash; число наблюдений, $D$ &ndash; количество переменных в модели. \n",
    "\n",
    "#### $MAPE$\n",
    "\n",
    "Средняя абсолютная процентная ошибка (англ. $\\textit{Mean Absolute Percentage Error}$, $MAPE$):\n",
    "\n",
    "$$MAPE = 100\\% \\cdot \\displaystyle\\frac{1}{M} \\sum_{i = 1}^M \\frac{|y_i - \\hat{y_i}|}{|y_i|},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Это коэффициент, не имеющий размерности, с очень простой интерпретацией. Его можно измерять в долях или процентах. Если у вас получилось, например, что $MAPE = 11.4\\%$, то это говорит о том, что ошибка составила $11,4\\%$ от фактических значений. Основная проблема данной ошибки &ndash; нестабильность.\n",
    "\n",
    "#### $RMSE$\n",
    "\n",
    "Корень из средней квадратичной ошибки (англ. $\\textit{Root Mean Squared Error}$, $RMSE$):\n",
    "\n",
    "$$RMSE = \\sqrt{\\displaystyle\\frac{1}{M}\\sum_{i = 1}^M (y_i - \\hat{y_i})^2},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Примерно такая же проблема, как и в $MAPE$: так как каждое отклонение возводится в квадрат, любое небольшое отклонение может значительно повлиять на показатель ошибки.\n",
    "\n",
    "#### $SMAPE$\n",
    "\n",
    "Cимметричная cредняя абсолютная процентная ошибка (англ. $\\textit{Symmetric MAPE}$, $SMAPE$):\n",
    "\n",
    "$$SMAPE = 100\\% \\cdot \\displaystyle\\frac{1}{M} \\sum_{i = 1}^M \\frac{|y_i - \\hat{y_i}|}{\\frac{|y_i| + |\\hat{y_i}|}{2}} = 100\\% \\cdot \\displaystyle\\frac{1}{M} \\sum_{i = 1}^M \\frac{2 \\cdot |y_i - \\hat{y_i}|}{|y_i| + |\\hat{y_i}|} = 200\\% \\cdot \\displaystyle\\frac{1}{M} \\sum_{i = 1}^M \\frac{|y_i - \\hat{y_i}|}{|y_i| + |\\hat{y_i}|},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Т.е. абсолютная разность между наблюдаемым и предсказанным значениями делится на полусумму их модулей. В отличие от обычной $MAPE$, симметричная имеет ограничение на диапазон значений. В приведённой формуле он составляет от $0$ до $200\\%$. Однако, поскольку диапазон от $0$ до $100\\%$ гораздо удобнее интерпретировать, часто используют формулу, где отсутствует деление знаменателя на $2$.\n",
    "\n",
    "Одной из возможных проблем $SMAPE$ является неполная симметрия, поскольку в разных диапазонах ошибка вычисляется неодинаково. Это иллюстрируется следующим примером: если $y_i = 100$ и $\\hat{y_i} = 110$, то $SMAPE = 4.76$, а если $y_i = 100$ и $\\hat{y_i} = 90$, то $SMAPE = 5.26$. Ограничение $SMAPE$ заключается в том, что, если наблюдаемое или предсказанное значение равно $0$, ошибка резко возрастет до верхнего предела ($200\\%$ или $100\\%$).\n",
    "\n",
    "#### $MSPE$\n",
    "\n",
    "Среднеквадратичная ошибка в процентах (англ. $\\textit{Mean Squared Percentage Error}$, $MSPE$) представляет собой относительную ошибку, где разность между наблюдаемым и фактическим значениями делится на наблюдаемое значение и выражается в процентах:\n",
    "\n",
    "$$MSPE = \\displaystyle\\frac{100\\%}{M}\\sum_{i = 1}^M \\left(\\frac{y_i - \\hat{y_i}}{y_i}\\right)^2,$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Проблемой при использовании $MSPE$ является то, что, если наблюдаемое значение выходной переменной равно $0$, значение ошибки становится неопределённым. $MSPE$ можно рассматривать как взвешенную версию $MSE$, где вес обратно пропорционален квадрату наблюдаемого значения. Таким образом, при возрастании наблюдаемых значений ошибка имеет тенденцию уменьшаться.\n",
    "\n",
    "#### $MASE$\n",
    "\n",
    "Средняя абсолютная масштабированная ошибка (англ. $\\textit{Mean Absolute Scaled Error}$, $MASE$) &ndash; это показатель, который позволяет сравнивать две модели. Если поместить $MAE$ для новой модели в числитель, а $MAE$ для исходной модели в знаменатель, то полученное отношение и будет равно $MASE$. Если значение $MASE$ меньше $1$, то новая модель работает лучше, если $MASE$ равно $1$, то модели работают одинаково, а если значение $MASE$ больше $1$, то исходная модель работает лучше, чем новая модель. Формула для расчета $MASE$ имеет вид:\n",
    "\n",
    "$$MASE = \\displaystyle\\frac{MAE_i}{MAE_j}.$$\n",
    "\n",
    "$MASE$ симметрична и устойчива к выбросам.\n",
    "\n",
    "#### $MRE$\n",
    "\n",
    "Средняя относительная ошибка (англ. $\\textit{Mean Relative Error}$, $MRE$) вычисляется по формуле:\n",
    "$$MRE = \\displaystyle\\frac{1}{M}\\sum_{i = 1}^M \\frac{|y_i - \\hat{y_i}|}{|y_i|},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Несложно увидеть, что данная мера показывает величину абсолютной ошибки относительно фактического значения выходной переменной (поэтому иногда эту ошибку называют также средней относительной абсолютной ошибкой, $MRAE$). Действительно, если значение абсолютной ошибки, скажем, равно $10$, то сложно сказать много это или мало. Например, относительно значения выходной переменной, равного $20$, это составляет $50\\%$, что достаточно много. Однако относительно значения выходной переменной, равного $100$, это будет уже $10\\%$, что является вполне нормальным результатом.  \n",
    "Очевидно, что при вычислении $MRE$ нельзя применять наблюдения, в которых $y_i = 0$.  \n",
    "Таким образом, $MRE$ позволяет более адекватно оценить величину ошибки, чем абсолютные ошибки. Кроме этого она является безразмерной величиной, что упрощает интерпретацию.\n",
    "\n",
    "#### $RMSLE$\n",
    "\n",
    "Среднеквадратичная логарифмическая ошибка (англ. $\\textit{Root Mean Squared Logarithmic Error}$, $RMSLE$) представляет собой $RMSE$, вычисленную в логарифмическом масштабе:\n",
    "\n",
    "$$RMSLE = \\sqrt{\\displaystyle\\frac{1}{M}\\sum_{i = 1}^M \\left(log(\\hat{y_i} + 1) - log(y_i + 1)\\right)^2},$$\n",
    "где $y_i$ &ndash; истинное значение целевого признака, $\\hat{y_i}$ &ndash; предсказанное значение целевого признака.  \n",
    "\n",
    "Константы, равные 1, добавляемые в скобках, необходимы чтобы не допустить обращения в 0 выражения под логарифмом, поскольку логарифм нуля не существует.  \n",
    "Известно, что логарифмирование приводит к сжатию исходного диапазона изменения значений переменной. Поэтому применение $RMSLE$ целесообразно, если предсказанное и фактическое значения выходной переменной различаются на порядок и больше.\n",
    "\n",
    "#### Сравнение метрик\n",
    "\n",
    "| Метрика     | Сильные стороны                                                                                                                                                                                   | Слабые стороны                                                                                                                                                                   |\n",
    "|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| $MSE$       | Позволяет подчеркнуть большие отклонения, простота вычисления.                                                                                                                                    | Имеет тенденцию занижать качество модели, чувствительна к выбросам. Сложность интерпретации из-за квадратичной зависимости.                                                      |\n",
    "| $RMSE$      | Простота интерпретации, поскольку измеряется в тех же единицах, что и целевая переменная.                                                                                                         | Имеет тенденцию занижать качество модели, чувствительна к выбросам.                                                                                                              |\n",
    "| $MSPE$      | Нечувствительна к выбросам. Хорошо интерпретируема, поскольку имеет линейный характер.                                                                                                            | Поскольку вклад всех ошибок отдельных наблюдений взвешивается одинаково, не позволяет подчёркивать большие и малые ошибки.                                                       |\n",
    "| $MAPE$      | Является безразмерной величиной, поэтому её интерпретация не зависит от предметной области.                                                                                                       | Нельзя использовать для наблюдений, в которых значения выходной переменной равны нулю.                                                                                           |\n",
    "| $SMAPE$     | Позволяет корректно работать с предсказанными значениями независимо от того больше они фактического, или меньше.                                                                                  | Приближение к нулю фактического или предсказанного значения приводит к резкому росту ошибки, поскольку в знаменателе присутствует как фактическое, так и предсказанное значения. |\n",
    "| $MASE$      | Не зависит от масштаба данных, является симметричной: положительные и отрицательные отклонения от фактического значения учитываются одинаково. Устойчива к выбросам. Позволяет сравнивать модели. | Сложность интерпретации.                                                                                                                                                         |\n",
    "| $MRE$       | Позволяет оценить величину ошибки относительно значения целевой переменной.                                                                                                                       | Неприменима для наблюдений с нулевым значением выходной переменной.                                                                                                              |\n",
    "| $RMSLE$     | Логарифмирование позволяет сделать величину ошибки более устойчивой, когда разность между фактическим и предсказанным значениями различается на порядок и выше                                    | Может быть затруднена интерпретация из-за нелинейности.                                                                                                                          |\n",
    "| $R^2$       | Универсальность, простота интерпретации.                                                                                                                                                          | Возрастает даже при включении в модель бесполезных переменных. Плохо работает когда входные переменные зависимы.                                                                 |\n",
    "| $R_{adj}^2$ | Корректно отражает вклад каждой переменной в модель.                                                                                                                                              | Плохо работает, когда входные переменные зависимы.                                                                                                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6455344-2f53-4560-964e-15eee542dfa2",
   "metadata": {},
   "source": [
    "### 5.7 Пример\n",
    "\n",
    "Задача: построить преобразование цвета в пространстве $RGB$. Имеются входные цвета &ndash; массив триплетов (троек) яркостей пиксела в каждом из $RGB$ каналов изображения, и выходные цвета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f31190-be0a-4af6-a14c-cff4535435f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# путь к папке с данными\n",
    "data_path = \"data\"\n",
    "\n",
    "with open(Path(data_path, 'chart_4096_colors.json')) as f:\n",
    "    data = json.load(f)\n",
    "colors = np.array(data['lut'])\n",
    "colors_in, colors_out = np.hsplit(colors, 2)\n",
    "colors_in, colors_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c2866-9963-4417-b682-1d7372020d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(colors)\n",
    "df.columns = ['$R_{in}$', '$G_{in}$', '$B_{in}$', '$R_{out}$', '$G_{out}$', '$B_{out}$']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dff38-ef82-4e84-803a-aaab40ce2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['$R_{in}$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b5799-5d8d-438f-ab22-2e1a5d8b3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(11, 4))\n",
    "\n",
    "ax[0].scatter(df['$R_{in}$'], df['$R_{out}$'], color='red')\n",
    "ax[1].scatter(df['$G_{in}$'], df['$G_{out}$'], color='green')\n",
    "ax[2].scatter(df['$B_{in}$'], df['$B_{out}$'], color='blue')\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.set_aspect('equal', adjustable='box')\n",
    "    axis.set_xlabel(df.columns[i])\n",
    "    axis.set_ylabel(df.columns[i + 3])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0573a-7692-47c0-be00-258519e63fc4",
   "metadata": {},
   "source": [
    "На графиках видно, что зависимость цвета в каждом цветовом канале имеет нелинейный характер. Поэтому сгенерируем дополнительные входные признаки, добавим полиномиальные члены:\n",
    "$$\n",
    "\\begin{matrix*}[l]\n",
    "r_{out} = \\displaystyle\\sum_{i = 0}^p w_i^R \\cdot r_{in}^i = w_0^R \\cdot 1 + w_1^R \\cdot r_{in} + w_2^R \\cdot r_{in}^2 + \\dots + w_p^R \\cdot r_{in}^p \\\\\n",
    "g_{out} = \\displaystyle\\sum_{i = 0}^p w_i^G \\cdot g_{in}^i = w_0^G \\cdot 1 + w_1^G \\cdot g_{in} + w_2^G \\cdot g_{in}^2 + \\dots + w_p^G \\cdot g_{in}^p \\\\\n",
    "b_{out} = \\displaystyle\\sum_{i = 0}^p w_i^B \\cdot b_{in}^i = w_0^B \\cdot 1 + w_1^B \\cdot b_{in} + w_2^B \\cdot b_{in}^2 + \\dots + w_p^B \\cdot b_{in}^p\n",
    "\\end{matrix*}\n",
    "$$\n",
    "\n",
    "То есть у нас есть 3 целевых признака ($R_{out}, G_{out}, B_{out}$) и набор входных признаков ($R_{in}, G_{in}, B_{in}$), дополненный сгенерированными признаками. Векторы коэффициентов наших полиномов $\\overrightarrow{w^R}, \\overrightarrow{w^G}, \\overrightarrow{w^B}$ &ndash; искомые решения.\n",
    "\n",
    "Для наглядности будем решать задачу только для одного цветового канала &ndash; красного:\n",
    "$$\n",
    "r_{out} = \\displaystyle\\sum_{i = 0}^p w_i^R \\cdot r_{in}^i = w_0^R \\cdot 1 + w_1^R \\cdot r_{in} + w_2^R \\cdot r_{in}^2 + \\dots + w_p^R \\cdot r_{in}^p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e8869-1747-4fb7-b4d7-8b018eb2ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = df.copy()\n",
    "df_ext = df_ext.div(255.0)\n",
    "df_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be867f6-1a1e-4c7a-9d05-47945d62a3ef",
   "metadata": {},
   "source": [
    "Добавим в датафрейм единичный признак &ndash; вектор свободных членов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed487ea-6280-412f-99bd-60d0c35c5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext['intercept'] = 1\n",
    "df_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97397ddd-f300-45c7-b340-c92a24d17545",
   "metadata": {},
   "source": [
    "Сгенерируем дополнительные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e34ef-6acc-4776-9bf5-cebdbdf558a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_deg = 3\n",
    "pd_l = [p for p in range(2, poly_deg + 1)]\n",
    "print(f'Polynomial degree list: {pd_l}')\n",
    "for p in pd_l:\n",
    "    df_ext[f'$R_{{in}}^{{{p}}}$'] = df_ext['$R_{in}$'].pow(p)\n",
    "df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbc4ab-78c8-44bc-b336-1cbf1f76ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2546ca4-150d-4d5e-8340-357f641526df",
   "metadata": {},
   "source": [
    "Сформируем список названий входных признаков: столбец свободных членов, $R_{in}$, $R_{in}^2$, $R_{in}^3$, отфильтруем датафрейм по списку признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85dcdc-3d26-4bc3-bdef-8c5112e0cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['intercept', '$R_{in}$']\n",
    "for p in pd_l:\n",
    "    columns.append(f'$R_{{in}}^{{{p}}}$')\n",
    "X = df_ext[columns]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31e939-a0bc-478c-a27b-f10318b5396f",
   "metadata": {},
   "source": [
    "Сформируем целевой признак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788d03e-24ec-4f30-811c-b990a4899ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ext['$R_{out}$']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3cd0f-88a7-4c65-b553-0a3be0d87c2e",
   "metadata": {},
   "source": [
    "#### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32106b4-21be-4f16-ab8c-90e7fbdfeeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE, r2_score as R2\n",
    "# Разделение данных на train и test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Создаем объект линейного регрессора\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Обучаем\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "# Получаем предсказание на тестовой выборке\n",
    "y_pred = regr.predict(test_X)\n",
    "\n",
    "max_len = 36\n",
    "# Коэффициенты регрессии\n",
    "print(f\"{'Коэффициенты регрессии:':<{max_len}} {regr.coef_}\")\n",
    "# MSE\n",
    "print(f\"{'Средняя квадратичная ошибка (MSE):':<{max_len}} {MSE(test_y, y_pred):.3f}\")\n",
    "# MAE\n",
    "print(f\"{'Средняя абсолютная ошибка (MAE):':<{max_len}} {MAE(test_y, y_pred):.3f}\")\n",
    "# R2 score\n",
    "print(f\"{'Коэффициент детерминации (R2 score):':<{max_len}} {R2(test_y, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882d12c-5c7e-4624-bf75-4b6043c023d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5a9b6-03b6-4b0a-91f3-364f0a57cee9",
   "metadata": {},
   "source": [
    "Рендер markdown ячейки, содержащий символы $\\mathrm \\LaTeX$, программно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72ebd4-8ef7-429a-b891-1e05402e4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "cols = train_X.columns.to_list()\n",
    "cols[0] = '1'\n",
    "s = f\"Полученное решение: $R_{{out}} = \"\n",
    "for col, val in zip(cols, regr.coef_):\n",
    "    s += f\"{val} \\cdot {col.strip('$')} +\"\n",
    "s = s[:-2]\n",
    "s += \"$\"\n",
    "\n",
    "display(Latex(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbf6a8-051b-4e0f-a838-1c023b97d011",
   "metadata": {},
   "source": [
    "Для построения кривой преобразования напишем вспомогательную функцию (дабы не плодить лишние данные, посчитаем сумму полиномиальных членов на лету):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980b46b-e0e8-4676-a167-a9592689c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_eval(x_in, w_in, poly_deg):\n",
    "    y_out = w_in[0] + w_in[1] * x_in\n",
    "    for p in range(2, poly_deg + 1):\n",
    "        y_out += w_in[p] * x_in ** p\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdb233-45ea-4c87-aac9-c6ff346d974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.array([x for x in range(256)]).reshape((-1, 1)) / 255.0\n",
    "y_pred_out = poly_eval(x_in, regr.coef_, poly_deg)\n",
    "# np.rint(np.hstack((x_in, y_pred_out)) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395041-4ed0-4c20-8ee4-731f93fdc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(11, 5))\n",
    "\n",
    "ax[0].scatter(train_X['$R_{in}$'], train_y, color='red')\n",
    "ax[0].set_xlabel('$R_{in}$')\n",
    "ax[0].set_ylabel('$R_{out}$')\n",
    "ax[0].plot(x_in, y_pred_out, color='blue')\n",
    "ax[0].set_aspect('equal', adjustable='box')\n",
    "ax[0].set_title('Train')\n",
    "\n",
    "ax[1].scatter(test_X['$R_{in}$'], test_y, color='red')\n",
    "ax[1].set_xlabel('$R_{in}$')\n",
    "ax[1].set_ylabel('$R_{out}$')\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "ax[1].set_title('Test')\n",
    "\n",
    "y_corr = poly_eval(X['$R_{in}$'].to_numpy(), regr.coef_, poly_deg)\n",
    "\n",
    "ax[2].scatter(X['$R_{in}$'], y_corr, color='red')\n",
    "ax[2].set_xlabel('$R_{in}$')\n",
    "ax[2].set_ylabel('$R_{out}$')\n",
    "ax[2].set_xlim(ax[1].get_xlim())\n",
    "ax[2].set_ylim(ax[1].get_ylim())\n",
    "ax[2].set_aspect('equal', adjustable='box')\n",
    "ax[2].set_title('Corrected')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8a9c3-234c-46e0-8231-bb902fe0d659",
   "metadata": {},
   "source": [
    "Объединим весь код в одну ячейку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049e12a-c3c3-463d-a8ad-26ac8a7c60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_deg = 11\n",
    "\n",
    "df_ext = df.copy()\n",
    "df_ext = df_ext.div(255.0)\n",
    "df_ext['intercept'] = 1\n",
    "\n",
    "pd_l = [p for p in range(2, poly_deg + 1)]\n",
    "for p in pd_l:\n",
    "    df_ext[f'$R_{{in}}^{{{p}}}$'] = df_ext['$R_{in}$'].pow(p)\n",
    "\n",
    "columns = ['intercept', '$R_{in}$']\n",
    "for p in pd_l:\n",
    "    columns.append(f'$R_{{in}}^{{{p}}}$')\n",
    "X = df_ext[columns]\n",
    "y = df_ext['$R_{out}$']\n",
    "\n",
    "# разделение данных на train и test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "y_pred = regr.predict(test_X)\n",
    "\n",
    "max_len = 36\n",
    "# Коэффициенты регрессии\n",
    "print(f\"Коэффициенты регрессии:\\n{regr.coef_}\")\n",
    "# MSE\n",
    "print(f\"{'Средняя квадратичная ошибка (MSE):':<{max_len}} {MSE(test_y, y_pred):.3f}\")\n",
    "# MAE\n",
    "print(f\"{'Средняя абсолютная ошибка (MAE):':<{max_len}} {MAE(test_y, y_pred):.3f}\")\n",
    "# R2 score\n",
    "print(f\"{'Коэффициент детерминации (R2 score):':<{max_len}} {R2(test_y, y_pred):.3f}\")\n",
    "\n",
    "x_in = np.array([x for x in range(256)]).reshape((-1, 1)) / 255.0\n",
    "y_pred_out = poly_eval(x_in, regr.coef_, poly_deg)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(11, 5))\n",
    "\n",
    "ax[0].scatter(train_X['$R_{in}$'], train_y, color='red')\n",
    "ax[0].set_xlabel('$R_{in}$')\n",
    "ax[0].set_ylabel('$R_{out}$')\n",
    "ax[0].plot(x_in, y_pred_out, color='blue')\n",
    "ax[0].set_aspect('equal', adjustable='box')\n",
    "ax[0].set_title('Train')\n",
    "\n",
    "ax[1].scatter(test_X['$R_{in}$'], test_y, color='red')\n",
    "ax[1].set_xlabel('$R_{in}$')\n",
    "ax[1].set_ylabel('$R_{out}$')\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "ax[1].set_title('Test')\n",
    "\n",
    "y_corr = poly_eval(X['$R_{in}$'].to_numpy(), regr.coef_, poly_deg)\n",
    "\n",
    "ax[2].scatter(X['$R_{in}$'], y_corr, color='red')\n",
    "ax[2].set_xlabel('$R_{in}$')\n",
    "ax[2].set_ylabel('$R_{corrected}$')\n",
    "ax[2].set_xlim(ax[1].get_xlim())\n",
    "ax[2].set_ylim(ax[1].get_ylim())\n",
    "ax[2].set_aspect('equal', adjustable='box')\n",
    "ax[2].set_title('Corrected')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "cols = train_X.columns.to_list()\n",
    "cols[0] = '1'\n",
    "s = f\"Полученное решение: \\n$$\\\\begin{{matrix*}}[l]R_{{out}} = &\"\n",
    "for i, (col, val) in enumerate(zip(cols, regr.coef_)):\n",
    "    if i % 3 == 0:\n",
    "        if i > 0:\n",
    "            s += f\"{[' -', ' +'][int(val > 0)]} \\\\\\ \\n&{['', '+'][int(val > 0)]}\"\n",
    "    else:\n",
    "        s += f\"{['', '+'][int(val > 0)]}\"\n",
    "    s += f\"{val} \\cdot {col.strip('$')}\"\n",
    "s += \"\\\\end{matrix*}$$\"\n",
    "display(Latex(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e16b-9d8d-4419-a862-ffc27d1ccbae",
   "metadata": {},
   "source": [
    "Хотя, на самом деле, решалась задача сложнее. Осуществлялся поиск преобразования вида $r_{out} = f(r_{in}, g_{in}, b_{in})$. Например, полином 3 степени:\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{out} = &w_0^R \\cdot 1 + w_1^R \\cdot r_{in} + w_2^R \\cdot g_{in} + w_3^R \\cdot b_{in} + w_4 \\cdot r_{in}^2 + w_5^R \\cdot r_{in} \\cdot g_{in} + w_6^R \\cdot g_{in}^2 + w_7^R \\cdot r_{in} \\cdot b_{in} + \\\\\n",
    "& + w_8^R \\cdot g_{in} \\cdot b_{in} + w_9^R \\cdot b_{in}^2 + w_{10}^R \\cdot r_{in}^3 + w_{11}^R \\cdot r_{in}^2 \\cdot g_{in} + w_{12}^R \\cdot r_{in} \\cdot g_{in}^2 + w_{13}^R \\cdot g_{in}^3 + w_{14}^R \\cdot r_{in}^2 \\cdot b_{in} + \\\\\n",
    "& + w_{15}^R \\cdot r_{in} \\cdot g_{in} \\cdot b_{in} + w_{16}^R \\cdot g_{in}^2 \\cdot b_{in} + w_{17}^R \\cdot r_{in} \\cdot b_{in}^2 + w_{18}^R \\cdot g_{in} \\cdot b_{in}^2 + w_{19}^R \\cdot b_{in}^3\n",
    "\\end{align}\n",
    "$$\n",
    "Или сокращенно:\n",
    "$$\n",
    "\\begin{matrix*}[l]\n",
    "r_{out} = \\displaystyle \\sum_{k_R + k_G + k_B \\leq p} w_i \\cdot r_{in}^{k_R} \\cdot g_{in}^{k_G} \\cdot b_{in}^{k_B}, \\\\\n",
    "i = \\overline{1, \\pmatrix{p + 3 \\\\ 3}} \\\\\n",
    "k_R, k_G, k_B = \\overline{0, p}\n",
    "\\end{matrix*}\n",
    "$$\n",
    "> Биномиальный коэффициент: $\\pmatrix{p + 3 \\\\ 3} = \\displaystyle\\frac{(p + 3)!}{3! \\cdot (p + 3 - 3)!} = \\frac{(p + 3)!}{3! \\cdot p!}$. При $p = 3$: $\\displaystyle\\frac{(3 + 3)!}{3! \\cdot 3!} = \\frac{6!}{3! \\cdot 3!} = \\displaystyle\\frac{1 \\cdot \\not{2} \\cdot \\not{3} \\cdot 4 \\cdot 5 \\cdot \\not{6}}{1 \\cdot \\not{2} \\cdot \\not{3} \\cdot 1 \\cdot \\not{2} \\cdot \\not{3}} = 20$, т.е. у нас 20 коэффициентов для всех сочетаний $r_{in}^{k_R} \\cdot g_{in}^{k_G} \\cdot b_{in}^{k_B}$ для полинома 3 степени ($k_R + k_G + k_B \\leq 3$), что мы и видели выше: $w_0,^R \\dotsc,w_{19}^R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3961f84-7031-40e7-8fb6-c37e3093dd64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.8 Датасет `Rain in Australia`\n",
    "\n",
    "Датасет содержит данные о метеонаблюдениях в Австралии, цель - прогнозирование дождя на следующий день. Целевой признак - `RainTomorrow`.\n",
    "\n",
    "| Column        | Meaning                                                                                                | Units               |\n",
    "|---------------|--------------------------------------------------------------------------------------------------------|---------------------|\n",
    "| Location      | The common name of the location of the weather station                                                 |                     |\n",
    "| MinTemp       | Minimum temperature in the 24 hours to 9am. Sometimes only known to the nearest whole degree.          | degrees Celsius     |\n",
    "| MaxTemp       | Maximum temperature in the 24 hours from 9am. Sometimes only known to the nearest whole degree.        | degrees Celsius     |\n",
    "| Rainfall      | Precipitation (rainfall) in the 24 hours to 9am. Sometimes only known to the nearest whole millimetre. | millimetres         |\n",
    "| Sunshine      | Bright sunshine in the 24 hours to midnight                                                            | hours               |\n",
    "| WindGustDir   | Direction of strongest gust in the 24 hours to midnight                                                | 16 compass points   |\n",
    "| WindGustSpeed | Speed of strongest wind gust in the 24 hours to midnight                                               | kilometres per hour |\n",
    "| WindDir9am    | Wind direction averaged over 10 minutes prior to 9 am                                                  | compass points      |\n",
    "| WindDir3pm    | Wind direction averaged over 10 minutes prior to 3 pm                                                  | compass points      |\n",
    "| WindSpeed9am  | Wind speed averaged over 10 minutes prior to 9 am                                                      | kilometres per hour |\n",
    "| WindSpeed3pm  | Wind speed averaged over 10 minutes prior to 3 pm                                                      | kilometres per hour |\n",
    "| Humidity9am   | Relative humidity at 9 am                                                                              | percent             |\n",
    "| Humidity3pm   | Relative humidity at 3 pm                                                                              | percent             |\n",
    "| Pressure9am   | Atmospheric pressure reduced to mean sea level at 9 am                                                 | hectopascals        |\n",
    "| Pressure3pm   | Atmospheric pressure reduced to mean sea level at 3 pm                                                 | hectopascals        |\n",
    "| Cloud9am      | Fraction of sky obscured by cloud at 9 am                                                              | eighths             |\n",
    "| Cloud3pm      | Fraction of sky obscured by cloud at 3 pm                                                              | eighths             |\n",
    "| Temp9am       | Temperature at 9 am                                                                                    | degrees Celsius     |\n",
    "| Temp3pm       | Temperature at 3 pm                                                                                    | degrees Celsius     |\n",
    "| RainToday     | The rain for that day was 1mm or more                                                                  | Yes or No           |\n",
    "| RainTomorrow  | The rain for that day was 1mm or more. The target variable to predict.                                 | Yes or No           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841c0b4-51b5-4959-b566-9a955902e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# путь к папке с данными\n",
    "data_path = \"..\\lecture_3\\data\"\n",
    "# датасет: Rain in Australia: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package\n",
    "df = pd.read_csv(Path(data_path, 'weatherAUS.csv'))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9f76f-393b-456c-9829-76d55e6afe41",
   "metadata": {},
   "source": [
    "### 5.9 Подготовка данных\n",
    "\n",
    "Более подробно процедуры предобработки данных описаны в лекциях 3-4.\n",
    "> Отличие: предобработка только датафрейма без разделения на $X$, $y$ и обучающую, тестовую выборки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e51a0e-b220-4cbc-b93c-84ed10fc03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Drop NaN in RainTomorrow\n",
    "df = df.drop(df[df['RainTomorrow'].isna()].index)\n",
    "# Cat cols\n",
    "cat_cols = [var for var in df.columns if df[var].dtype == 'object']\n",
    "cat_null = df[cat_cols].isnull().sum()\n",
    "cat_null_mode = df[cat_null[cat_null > 0].index].mode()\n",
    "print(f\"Cat cols with NaNs mode: {cat_null_mode}\")\n",
    "# Fill NaNs\n",
    "for col in cat_cols:\n",
    "    df.fillna({col: df[col].mode()[0]}, inplace=True)\n",
    "# Print cat cols cardinality\n",
    "len_max = max([len(col) for col in cat_cols])\n",
    "for col in cat_cols:\n",
    "    print(f\"{col:<{len_max}} labels: {len(df[col].unique())}\")\n",
    "# Split Date into Day, Month, Year\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "cat_cols.remove('Date')\n",
    "# Num cols\n",
    "num_cols = [var for var in df.columns if not df[var].dtype == 'object']\n",
    "# Fill NaNs\n",
    "for col in num_cols:\n",
    "    df.fillna({col: df[col].median()}, inplace=True)\n",
    "# Encode Location\n",
    "df_loc_dummy = pd.get_dummies(df.Location, prefix='Location')\n",
    "df = df.drop('Location', axis = 1)\n",
    "df = df.join(df_loc_dummy)\n",
    "# Cat cols left\n",
    "cat_left = [var for var in df.columns if df[var].dtype == 'object']\n",
    "cat_left = ['WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "# Encode cat cols\n",
    "df = pd.get_dummies(data=df, columns=cat_left, drop_first=False)\n",
    "# Replace values\n",
    "# df.replace({'RainToday': {'No': 0, 'Yes': 1}}, inplace = True)\n",
    "# df.replace({'RainTomorrow': {'No': 0, 'Yes': 1}}, inplace = True)\n",
    "df.replace({'RainToday': {'No': '0', 'Yes': '1'}}, inplace = True)\n",
    "df.replace({'RainTomorrow': {'No': '0', 'Yes': '1'}}, inplace = True)\n",
    "df['RainToday'] = df['RainToday'].astype(int)\n",
    "df['RainTomorrow'] = df['RainTomorrow'].astype(int)\n",
    "\n",
    "num_cols_ext = deepcopy(num_cols)\n",
    "num_cols_ext.append('RainToday')\n",
    "# Scale data\n",
    "all_cols = list(df.columns)\n",
    "mm_scaler = MinMaxScaler()\n",
    "features_scaled = mm_scaler.fit_transform(df[all_cols])\n",
    "df_scaled = pd.DataFrame(features_scaled, columns=all_cols)\n",
    "\n",
    "# Outliers bounds\n",
    "def get_bounds(dataframe, col):\n",
    "    iqr = dataframe[col].quantile(0.75) - dataframe[col].quantile(0.25)\n",
    "    lower_bound = dataframe[col].quantile(0.25) - 1.5 * iqr\n",
    "    upper_bound = dataframe[col].quantile(0.75) + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "num_cols_clean = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
    "\n",
    "bounds_dict = dict()\n",
    "\n",
    "for col in num_cols_clean:\n",
    "    lb, ub = get_bounds(df_scaled, col)\n",
    "    bounds_dict[col] = [lb, ub]\n",
    "    print(f\"{col:<13} outliers are values < {lb:.2f} or > {ub:.2f}\")\n",
    "\n",
    "# Clean outliers\n",
    "def clean_data(df, bounds_dict: dict):\n",
    "    df_clean = deepcopy(df)\n",
    "    print(df_clean.shape)\n",
    "\n",
    "    for k, v in bounds_dict.items():\n",
    "        arr = np.array((df_clean[k] > v[0]) & (df_clean[k] < v[1])).reshape((-1, 1))\n",
    "        print(f\"{k}: bounds: {v}\")\n",
    "        print(f\"  old: {df_clean[k].shape[0]}, new: {np.count_nonzero(arr)}, diff: {np.count_nonzero(arr) - df_clean[k].shape[0]}\")\n",
    "        df_clean = df_clean[(df_clean[k] > v[0]) & (df_clean[k] < v[1])]\n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_data(df_scaled, bounds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058d705-2c68-4f70-8e0f-7da1c5499e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575f983-a160-4565-b5ef-f55d86f9f9a0",
   "metadata": {},
   "source": [
    "Рассмотрим количественные признаки без закодированных категориальных. Исключим признаки `Year`, `Month`, `Day`, `RainToday`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe4247-a2af-4379-8e20-8223f481ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_ext_reg = [x for x in num_cols_ext if x not in ['Year', 'Month', 'Day', 'RainToday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be670e8-2779-452a-9731-bf0bf4071244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[num_cols_ext_reg].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368c568-f8c5-42e6-962d-79900238ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[num_cols_ext_reg].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429c118-aff4-44e0-a0f5-102548c88a09",
   "metadata": {},
   "source": [
    "### 5.10 Линейная регрессия для `Rain in Australia`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b66ce9-4af7-4cb1-843b-24bca755ef88",
   "metadata": {},
   "source": [
    "Пусть целевой признак &ndash; `MinTemp` &ndash; количество осадков в мм. Построим график `sns.pairplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf051d77-2ca4-4b28-ab3a-fc793ba36ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5694119-1541-4718-8ef1-e50471dde075",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'MinTemp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ea855-4ff6-4253-8e6d-0aff55f58067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/67853551\n",
    "# https://stackoverflow.com/a/74662884\n",
    "fig = sns.FacetGrid(pd.DataFrame(num_cols_ext_reg), col=0, col_wrap=3, sharex=False)\n",
    "\n",
    "for ax, varx in zip(fig.axes, num_cols_ext_reg):\n",
    "    sns.scatterplot(data=df_clean[num_cols_ext_reg], x=varx, y=target, ax=ax)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d80b0-fcf2-4853-9a0d-08910a70562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean[num_cols_ext_reg].drop([target], axis=1)\n",
    "y = df_clean[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9babf-c9b0-40b0-b505-172be73ece31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на train и test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_X, train_y)\n",
    "\n",
    "y_pred = regr.predict(test_X)\n",
    "\n",
    "max_len = 36\n",
    "# Коэффициенты регрессии\n",
    "print(f\"Коэффициенты регрессии:\\n{regr.coef_}\")\n",
    "# MSE\n",
    "print(f\"{'Средняя квадратичная ошибка (MSE):':<{max_len}} {MSE(test_y, y_pred):.3f}\")\n",
    "# MAE\n",
    "print(f\"{'Средняя абсолютная ошибка (MAE):':<{max_len}} {MAE(test_y, y_pred):.3f}\")\n",
    "# R2 score\n",
    "print(f\"{'Коэффициент детерминации (R2 score):':<{max_len}} {R2(test_y, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014816b-0863-46de-9ebb-d6a87a8b8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(columns=X.columns, data=np.reshape(regr.coef_, (1, -1)))\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef513ac-1088-445f-a51f-bea149aef847",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.hist(y_pred - test_y, bins=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ac44d-ed42-4884-b392-f324a96bf37b",
   "metadata": {},
   "source": [
    "### 5.11 Recursive Feature Elimination (RFE)\n",
    "\n",
    "Источник:\n",
    "- [Ранжирование признаков с помощью Recursive Feature Elimination в Scikit-Learn @ Хабр](https://habr.com/ru/companies/otus/articles/528676/)\n",
    "\n",
    "Первым элементом, необходимым для рекурсивного исключения признаков (recursive feature elimination), является оценщик, например, линейная модель или дерево решений.  \n",
    "\n",
    "У таких моделей есть коэффициенты для линейных моделей и важности признаков в деревьях решений. Для выбора оптимального количества признаков нужно обучить оценщика и выбрать признаки с помощью коэффициентов или значений признаков. Наименее важные признаки будут удаляться. Этот процесс будет повторяться рекурсивно до тех пор, пока не будет получено оптимальное число признаков.  \n",
    "\n",
    "Мы используем [Pipeline](https://scikit-learn.org/stable/modules/compose.html) для преобразования данных. В `Pipeline` мы указываем $RFE$ для шага отбора признаков и модель, которая будет использоваться на следующем шаге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172908a5-cffb-4924-bfd0-a01f7a79c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=linear_model.LinearRegression(), n_features_to_select=7)\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[('rfe', rfe), ('model', model)])\n",
    "rf_pipeline.fit(train_X, train_y)\n",
    "\n",
    "y_pred = rf_pipeline.predict(test_X)\n",
    "\n",
    "max_len = 36\n",
    "print(f\"Выбранные признаки:\\n{rf_pipeline.named_steps['rfe'].get_feature_names_out()}\")\n",
    "# Коэффициенты регрессии\n",
    "print(f\"Коэффициенты регрессии:\\n{model.coef_}\")\n",
    "# MSE\n",
    "print(f\"{'Средняя квадратичная ошибка (MSE):':<{max_len}} {MSE(test_y, y_pred):.3f}\")\n",
    "# MAE\n",
    "print(f\"{'Средняя абсолютная ошибка (MAE):':<{max_len}} {MAE(test_y, y_pred):.3f}\")\n",
    "# R2 score\n",
    "print(f\"{'Коэффициент детерминации (R2 score):':<{max_len}} {R2(test_y, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368075cf-cbd0-4e10-a658-b9c65cfbe310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=linear_model.LinearRegression(), n_features_to_select=7)\n",
    "\n",
    "parameters = {'alpha': np.arange(0.01, 0.1, 0.01), 'l1_ratio': np.arange(0.01, 0.1, 0.01)}\n",
    "elastic_net = linear_model.ElasticNet()\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net, parameters)\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[('rfe', rfe), ('model', grid_search)])\n",
    "rf_pipeline.fit(train_X, train_y)\n",
    "\n",
    "y_pred = rf_pipeline.predict(test_X)\n",
    "\n",
    "max_len = 36\n",
    "print(f\"Выбранные признаки:\\n{rf_pipeline.named_steps['rfe'].get_feature_names_out()}\")\n",
    "print(f\"Параметры лучшей модели:\\n{grid_search.best_params_}\")\n",
    "# Коэффициенты регрессии\n",
    "print(f\"Коэффициенты регрессии:\\n{grid_search.best_estimator_.coef_}\")\n",
    "# MSE\n",
    "print(f\"{'Средняя квадратичная ошибка (MSE):':<{max_len}} {MSE(test_y, y_pred):.3f}\")\n",
    "# MAE\n",
    "print(f\"{'Средняя абсолютная ошибка (MAE):':<{max_len}} {MAE(test_y, y_pred):.3f}\")\n",
    "# R2 score\n",
    "print(f\"{'Коэффициент детерминации (R2 score):':<{max_len}} {R2(test_y, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb65a4-3514-49d7-953d-92b938f3fb58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
