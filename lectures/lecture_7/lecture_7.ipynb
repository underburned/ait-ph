{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7182c113-d196-4a94-b168-ab98c2f22acd",
   "metadata": {},
   "source": [
    "# Технологии искусственного интеллекта\n",
    "\n",
    "© Петров М.В., старший преподаватель кафедры суперкомпьютеров и общей информатики, Самарский университет\n",
    "\n",
    "## Лекция 7. Перцептрон\n",
    "\n",
    "### Содержание\n",
    "\n",
    "1. [Введение](#7.1-Введение)\n",
    "2. [Нейронная сеть](#7.2-Нейронная-сеть)\n",
    "3. [Перцептрон](#7.3-Перцептрон)\n",
    "4. [Back propagation - алгоритм обучения по методу обратного распространения](#7.4-Back-propagation--алгоритм-обучения-по-методу-обратного-распространения)\n",
    "5. [Датасет `Rain in Australia`](#7.5-Датасет-Rain-in-Australia)\n",
    "6. [Подготовка данных](#7.6-Подготовка-данных)\n",
    "7. [Классификация](#7.7-Классификация)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9a808-2ad5-4b6d-8cdc-5a34b5378272",
   "metadata": {},
   "source": [
    "### 7.1 Введение\n",
    "\n",
    "Источники:\n",
    " - [Нейронная сеть](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)\n",
    " - [Структура и принцип работы полносвязных нейронных сетей](https://proproprogs.ru/neural_network/struktura-i-princip-raboty-polnosvyaznyh-neyronnyh-setey), [YouTube](https://www.youtube.com/watch?v=VqChpNNYZ8Q&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - [Персептрон - возможности классификации образов, задача XOR](https://proproprogs.ru/neural_network/perseptron-vozmozhnosti-klassifikacii-obrazov-zadacha-xor), [YouTube](https://www.youtube.com/watch?v=t9QfcFNkG58&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - [Back propagation - алгоритм обучения по методу обратного распространения](https://proproprogs.ru/neural_network/back-propagation-algoritm-obucheniya-po-metodu-obratnogo-rasprostraneniya), [YouTube](https://www.youtube.com/watch?v=UXB9bFj-UA4&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - Тарик Рашид, \"Создаем нейронную сеть\"\n",
    " - [Perceptron @ sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    " - [MLPClassifier @ sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "\n",
    "\n",
    "#### Краткая история\n",
    "\n",
    " - 1943 г. Уоррен Мак-Каллок и Уолтер Питтс формализуют понятие нейронной сети в фундаментальной статье о логическом исчислении идей и нервной активности. Они выдвинули предположение, что нейроны можно упрощённо рассматривать как устройства, оперирующие двоичными числами, и назвали эту модель \"пороговой логикой\". Подобно своему биологическому прототипу нейроны Мак-Каллока-Питтса были способны обучаться путём подстройки параметров, описывающих синаптическую проводимость. Исследователи предложили конструкцию сети из электронных нейронов и показали, что подобная сеть может выполнять практически любые вообразимые числовые или логические операции. Мак-Каллок и Питтс предположили, что такая сеть в состоянии также обучаться, распознавать образы, обобщать, т.е. обладает всеми чертами интеллекта.\n",
    " - 1948 г. Опубликована книга Н. Винера о кибернетике. Основной идеей стало представление сложных биологических процессов математическими моделями.\n",
    " - 1949 г. Дональд Олден Хебб предлагает первый алгоритм обучения. Он первым предположил, что обучение заключается в первую очередь в изменениях силы синаптических связей. Теория Хебба считается типичным случаем самообучения, при котором испытуемая система спонтанно обучается выполнять поставленную задачу без вмешательства со стороны экспериментатора.\n",
    " - 1958 г. Фрэнк Розенблатт изобретает однослойный перцептрон и демонстрирует его способность решать задачи классификации. Перцептрон использовали для распознавания образов, прогнозирования погоды.\n",
    " - 1963 г. В Институте проблем передачи информации АН СССР Александром Павловичем Петровым проводится исследование задач, «трудных» для перцептрона.\n",
    " - 1969 г. Марвин Ли Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он не способен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений. Второй важной проблемой было то, что компьютеры не обладали достаточной вычислительной мощностью, чтобы эффективно обрабатывать огромный объём вычислений, необходимый для больших нейронных сетей.\n",
    " - 1974 г. Пол Вербос и Александр Иванович Галушкин А. И. одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов.\n",
    " - 1975 г. Кунихико Фукусима представляет когнитрон &ndash; самоорганизующуюся сеть, предназначенную для инвариантного распознавания образов, но это достигается только при помощи запоминания практически всех состояний образа.\n",
    " - 1982 г. Джон Джозеф Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (сеть Хопфилда). Теуво Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.\n",
    " - 1986 г. Дэвидом Румельхартом, Джеффри Хинтоном и Рональдом Вильямсом, а также независимо и одновременно С.И. Барцевым и В.А. Охониным, развит метод обратного распространения ошибки.\n",
    " - 1989 г. Ян Лекун представил первую сверточную нейронную сеть (LeNet), способную распознавать рукописные цифры с хорошей скоростью и точностью распознавания. [Демонстрация работы](https://www.youtube.com/watch?v=FwFduRA_L6Q).\n",
    " - 2007 г. Джеффри Хинтоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Хинтон при обучении нижних слоёв сети использовал ограниченную машину Больцмана (RBM — Restricted Boltzmann Machine). По Хинтону необходимо использовать много примеров распознаваемых образов (например, множество лиц людей на разных фонах). После обучения получается готовое быстро работающее приложение, способное решать конкретную задачу (например, осуществлять поиск лиц на изображении). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448ad0d-ac81-4932-aa5f-b0d41548954f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.2 Нейронная сеть\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Neuron-rus.svg/1920px-Neuron-rus.svg.png\" width=\"600\" title=\"Структура нейрона\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 1 &ndash; Структура нейрона\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Мозг состоит из большого количества нейронов, связанных между собой дендритами и аксонами. Дендриты - это отростки, по которым нервные импульсы передаются к телу нейрона. Эти отростки сильно ветвятся. У нейрона может быть несколько дендритов. Аксон - это отросток, по которому импульсы передаются от тела клетки.\n",
    "\n",
    "Фрэнк Розенблатт предложил перцептрон, схема которого вдохновлена биологическим прототипом:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_1.svg\" width=\"600\" title=\"Перцептрон\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 2 &ndash; Перцептрон\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "#### Полносвязная нейронная сеть прямого распространения\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/full_connected_network.svg\" width=\"1000\" title=\"Полносвязная нейронная сеть\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 3 &ndash; Полносвязная нейронная сеть\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    " - Каждый нейрон текущего слоя связан с каждым нейроном следующего слоя.\n",
    " - Прямое распространение - сигнал распространяется от входного слоя к выходному, не образуя обратных связей.  \n",
    "\n",
    "Каждая связь между нейронами имеет определенный вес $\\omega_{ij}$ - весовой коэффициент, связанный с сигналом, который передается от $i$-го нейрона текущего слоя к $j$-му нейрону следующего слоя:  \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/full_connected_network_2.svg\" width=\"1000\" title=\"Полносвязная нейронная сеть\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 4 &ndash; Полносвязная нейронная сеть\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Сам по себе нейрон – это сумматор входных сигналов, который, затем, пропускает сумму через функцию $f(x)$, называемую функцией активации. Выходное значение этой функции и есть выходное значение нейрона.\n",
    "В концепции персептрона функции активации выбираются пороговыми:\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    a,  & \\quad \\text{если } x \\geq T\\\\\n",
    "    b,  & \\quad \\text{если } x < T\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "#### Пример\n",
    "\n",
    "Вы решили сходить в кино. Выбор вы осуществляете по трем параметрам:\n",
    " - наличие в фильме любимого актера;\n",
    " - интересное описание сюжета;\n",
    " - жанр фильма (вы недолюбливаете детективы, например).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/cinema_1.svg\" width=\"1000\" title=\"Поход в кино\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 4 &ndash; Поход в кино\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Сценарий 1:\n",
    "- [x] Любимый актер\n",
    "- [x] Интересный сюжет\n",
    "- [x] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 1 - 0.5 \\cdot 1) = f(0.5) = 1$. Получаем граничное значение, ну, можно и сходить.\n",
    "\n",
    "Сценарий 2:\n",
    "- [x] Любимый актер\n",
    "- [ ] Интересный сюжет\n",
    "- [ ] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 0 - 0.5 \\cdot 0) = f(0.5) = 1$. Любимый актер присутствует, сюжет не ахти, зато не нудный детектив. Тоже сойдет.\n",
    "\n",
    "Сценарий 3:\n",
    "- [x] Любимый актер\n",
    "- [ ] Интересный сюжет\n",
    "- [x] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 0 - 0.5 \\cdot 1) = f(0) = 0$. Любимый актер присутствует, сюжета нет, да еще и нелюбимый жанр. Неа.\n",
    "\n",
    "Максимальная мотивация сходить в кино достигается при:\n",
    "- [x] Любимый актер\n",
    "- [x] Интересный сюжет\n",
    "- [ ] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 1 - 0.5 \\cdot 0) = f(1) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15c3c4-3a22-496a-99a7-2ba9dbd258ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "def act(x):\n",
    "    return 0 if x < 0.5 else 1\n",
    " \n",
    "def go(fav_actor, interesting_plot, detective_genre):\n",
    "    x = np.array([fav_actor, interesting_plot, detective_genre])\n",
    "    w = [0.5, 0.5, -0.5]\n",
    "    weight = np.array(w)\n",
    "    sum_end = np.dot(weight, x)\n",
    "    y = act(sum_end)\n",
    "    return y\n",
    "\n",
    "vars = np.array(list(itertools.chain(*itertools.product([0, 1], repeat=3))))\n",
    "vars = np.reshape(vars, (-1,3))\n",
    "\n",
    "df = pd.DataFrame(vars, columns = ['fav_actor', 'interesting_plot', 'detective_genre'])\n",
    "df['go'] = df.apply(lambda row : go(row['fav_actor'], row['interesting_plot'], row['detective_genre']), axis = 1)\n",
    "df['go'] = df['go'].astype(str)\n",
    "df['go'].replace({str(0): 'Кино ноуп', str(1): 'Го в кино'}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c08de-42b0-4c84-84fa-9743631d2ce7",
   "metadata": {},
   "source": [
    "Пусть наличие любимого актера для нас - главное. Но и в целом, на интересное кино с ноунеймами без детективных арок сюжета мы готовы сходить. Добавим еще один слой.  \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/cinema_2.svg\" width=\"1000\" title=\"Поход в кино\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 5 &ndash; Поход в кино\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2df6f-ef0c-4583-bbe4-cae69ae90c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "def act(x):\n",
    "    return 0 if x < 0.5 else 1\n",
    " \n",
    "def go(fav_actor, interesting_plot, detective_genre):\n",
    "    x = np.array([fav_actor, interesting_plot, detective_genre])\n",
    "    w11 = [1, 0.5, -0.4]\n",
    "    w12 = [0, 0.4, -0.5]\n",
    "    weight1 = np.array([w11, w12])   # матрица 2x3\n",
    "    weight2 = np.array([1, -1])      # вектор 1х3\n",
    " \n",
    "    sum_hidden = np.dot(weight1, x)  # вычисляем сумму на входах нейронов скрытого слоя\n",
    "    out_hidden = np.array([act(x) for x in sum_hidden])\n",
    " \n",
    "    sum_end = np.dot(weight2, out_hidden)\n",
    "    y = act(sum_end)\n",
    " \n",
    "    return y\n",
    "\n",
    "vars = np.array(list(itertools.chain(*itertools.product([0, 1], repeat=3))))\n",
    "vars = np.reshape(vars, (-1,3))\n",
    "\n",
    "df = pd.DataFrame(vars, columns = ['fav_actor', 'interesting_plot', 'detective_genre'])\n",
    "df['go'] = df.apply(lambda row : go(row['fav_actor'], row['interesting_plot'], row['detective_genre']), axis = 1)\n",
    "df['go'] = df['go'].astype(str)\n",
    "df['go'].replace({str(0): 'Кино ноуп', str(1): 'Го в кино'}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013ea18-8df8-4874-b540-ae2c3565a2f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.3 Перцептрон\n",
    "\n",
    "Для простоты рассмотрим простейший перцептрон для задачи классификации двух классов образов, представленных двумя характеристиками $x_1, x_2$:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_2.svg\" width=\"500\" title=\"Простейший перцептрон\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 6 &ndash; Простейший перцептрон\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "С активационной функцией:\n",
    "$$f(x)=\\begin{cases}\n",
    "    1, &x\\geq 0 &\\rightarrow C_1\\\\\n",
    "    -1, &x < 0 &\\rightarrow C_2\n",
    "\\end{cases}$$\n",
    "\n",
    "То есть, если значение суммы больше или равно 0, то вектор принадлежит классу 1:\n",
    "$$[x_1, x_2]{^{\\mkern-1.5mu\\mathsf{T}}} \\in C_1$$\n",
    "Иначе классу 2:\n",
    "$$[x_1, x_2]{^{\\mkern-1.5mu\\mathsf{T}}} \\in C_2$$\n",
    "\n",
    "Далее, из вида активационной функции видно, что граница разделения двух классов проходит на уровне 0. То есть, если:\n",
    "$$f(x)=\\begin{cases}\n",
    "    \\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 \\geq 0 &\\rightarrow C_1\\\\\n",
    "    \\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 < 0 &\\rightarrow C_2\n",
    "\\end{cases}$$\n",
    "\n",
    "Значит, сумма:\n",
    "$$\\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 = 0$$\n",
    "\n",
    "определяет границу разделения одного класса образов от другого. Ее еще можно записать в виде:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1$$\n",
    "\n",
    "То есть, это прямая с угловым коэффициентом\n",
    "$$k = - \\frac{\\omega_1}{\\omega_2}$$\n",
    "проходящая через начало системы координат:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_3.svg\" width=\"500\" title=\"Разделяющая прямая\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 7 &ndash; Разделяющая прямая\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "И все точки по одну сторону от этой прямой будут относиться к одному классу, а по другую сторону – к другому классу. Такая прямая получила название разделяющей прямой (в многомерном случае она превращается в гиперплоскость и называется разделяющей гиперплоскостью). Этот двумерный график хорошо демонстрирует возможность правильной классификации простейшим персептроном только линейно-разделимых образов.\n",
    "\n",
    "#### Пример.\n",
    "Пусть у нас есть два класса линейно-разделимых образов разделяющей прямой:\n",
    "$$x_2 = 1 \\cdot x_1$$\n",
    "\n",
    "В этом случае, для корректной классификации мы должны выбрать веса нейронной сети равными, но с противоположными знаками:\n",
    "$$k = 1 = - \\frac{\\omega_1}{\\omega_2} \\Rightarrow \\omega_1 = -0.5, \\omega_2 = 0.5$$\n",
    "\n",
    "Предположим, что все наши образы сдвигаются вверх по оси $x_2$ на некоторую величину:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_4.svg\" width=\"500\" title=\"Разделяющая прямая\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 8 &ndash; Разделяющая прямая\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Теперь наша разделяющая прямая не сможет верно классифицировать такие образы, т.к. она проходит через начало координат. И как бы мы ее ни крутили, корректного разделения не получится. Необходимо смещение. Поэтому, в НС дополнительно определяют еще один вход для смещения разделяющей гиперплоскости. В английской литературе он называется `bias` (перевести можно как `порог`).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_5.svg\" width=\"750\" title=\"Простейший перцептрон со смещением\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 9 &ndash; Простейший перцептрон со смещением\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "С этим дополнительным входом, наша прямая принимает вид:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1 - \\frac{\\omega_3}{\\omega_2} \\cdot 1$$\n",
    "\n",
    "То есть, мы можем теперь сдвинуть ее на любое требуемое значение. Пусть все образы сдвинуты вверх по оси $x_2$ на величину $b$. Тогда третий весовой коэффициент НС следует выбрать из уравнения:\n",
    "$$-\\frac{\\omega_3}{\\omega_2} = b \\Rightarrow \\omega_3 = -b \\cdot \\omega_2$$\n",
    "\n",
    "### Задача XOR\n",
    "\n",
    "Рассмотренная нами НС с одним нейроном может классифицировать только линейно-разделимые образы. Однако, на практике чаще встречаются более сложные задачи. Например, представим, что классы наших образов распределены следующим образом:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_1.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 10\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Здесь невозможно провести одну линию для их правильной классификации. Как тогда быть? Например, провести две линии, вот так:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_2.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 11\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "И все, что будет попадать между ними – отнесем к первому классу, а за их пределами – ко второму классу. Что это за НС, которая способна на такие операции? В действительности, все просто: каждая разделительная линия может быть представлена отдельным нейроном, а затем, результат их классификации объединяется результирующим нейроном выходного слоя:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_3.svg\" width=\"750\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 12\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Для простоты будем полагать, что на входы подаются только значения $0$ или $1$:\n",
    "$$x_1, x_2 \\in [0, 1]$$\n",
    "\n",
    "Тогда все наши образы будут лежать в углах квадрата:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "  0 & 0 & C_2 \\\\\n",
    "  0 & 1 & C_1 \\\\\n",
    "  1 & 0 & C_1 \\\\\n",
    "  1 & 1 & C_2\n",
    " \\end{matrix}\n",
    "$$\n",
    "\n",
    "Если задать $C_2 = 0, C_1 = 1$, то получаем таблицу истинности для битовой операции `XOR` (`исключающее ИЛИ`). Поэтому в литературе задача разделения таких образов получила название `задачи XOR`.\n",
    "Активационная функция каждого нейрона будет иметь вид:\n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "    1, & x \\geq 0\\\\\n",
    "    0, & x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Осталось определить веса связей НС для решения поставленной задачи классификации. Для начала, положим, что первый нейрон скрытого слоя будет формировать границу:\n",
    "$$x_2 = -1 \\cdot x_1 + 1.5$$\n",
    "\n",
    "Учитывая нашу формулу:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1 - \\frac{\\omega_3}{\\omega_2} \\cdot 1$$\n",
    "\n",
    "Веса входов первого нейрона для $x_1, x_2$ можно взять равными:\n",
    "$$\\omega_1 = \\omega_2 = 1$$\n",
    "\n",
    "а вес третьей связи:\n",
    "$$\\omega_3 = - b \\cdot \\omega_2 = - 1.5$$\n",
    "\n",
    "Получили прямую, которая формирует следующее разделение на плоскости:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_4.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 13\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Второй нейрон скрытого слоя будет формировать разделения прямой:\n",
    "$$x_2 = -1 \\cdot x_1 + 0.5$$\n",
    "\n",
    "и веса его связей можно взять равными:\n",
    "$$\\omega_1 = \\omega_2 = 1, \\omega_3 = -0.5$$\n",
    "\n",
    "Получаем следующую картину:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_5.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 14\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Теперь нам нужно объединить результаты их работы, чтобы получилась следующая разделяющая область:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_6.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 15\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Для этого из второго вычтем первое:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_7.svg\" width=\"1000\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 16\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Для надежности сместим эти значения на -0.5 и окончательно получим результат:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_8.svg\" width=\"500\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 17\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Веса в нашей НС будут следующими:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/perceptron_xor_9.svg\" width=\"750\" title=\"Untitled\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 18\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Как видим, результаты получаются именно такими, какие мы и ожидали, классификация задачи `XOR` выполнена успешно благодаря добавлению скрытого слоя нейронов.  \n",
    "\n",
    "Этот пример хорошо показывает, что добавляя новые нейроны, мы можем получать все более сложные формы разделяющих выпуклых областей, полученные комбинацией разделяющих линий или гиперплоскостей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7beb649-a3e2-4250-9e6f-d53942a9fe05",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.4 Back propagation - алгоритм обучения по методу обратного распространения\n",
    "\n",
    "При увеличении числа нейронов и связей ручной подбор весов НС становится попросту невозможным и возникает задача нахождения весовых коэффициентов связей НС. Этот процесс и называют обучением нейронной сети.  \n",
    "Один из распространенных подходов к обучению заключается в последовательном предъявлении НС векторов наблюдений и последующей корректировки весовых коэффициентов так, чтобы выходное значение совпадало с требуемым. Это называется `обучение с учителем`, так как для каждого вектора мы знаем нужный ответ и именно его требуем от нашей НС.  \n",
    "\n",
    "Теперь, главный вопрос: как построить алгоритм, который бы наилучшим образом находил весовые коэффициенты. Наилучший – это значит, максимально быстро и с максимально близкими выходными значениями для требуемых откликов. В общем случае эта задача не решена. Нет универсального алгоритма обучения. Поэтому, лучшее, что мы можем сделать – это выбрать тот алгоритм, который хорошо себя зарекомендовал в прошлом. Основной «рабочей лошадкой» здесь является алгоритм `back propagation` (`обратного распространения ошибки`), который, в свою очередь, базируется на алгоритме `градиентного спуска`.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/gd.gif\" width=\"500\" title=\"GD 1\"/>\n",
    "  <br>\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif\" width=\"500\" title=\"GD 2\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 19 &ndash; Градиентный спуск\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Чтобы все лучше понять, предположим, что у нас имеется вот такая полносвязная НС прямого распространения с весами связей, выбранными произвольным образом в диапазоне от $[-0.5, 0.5]$. Здесь верхний индекс показывает принадлежность к тому или иному слою сети. Также, каждый нейрон имеет некоторую активационную функцию $f(x)$:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/back_prop_1.svg\" width=\"750\" title=\"НС\"/>\n",
    "  <p style=\"text-align: center\">\n",
    "    Рисунок 20 &ndash; НС\n",
    "  </p>\n",
    "</div>  \n",
    "\n",
    "На первом шаге делается прямой проход по сети. Мы пропускаем вектор наблюдения $[x_1, x_2]$ через эту сеть, и запоминаем все выходные значения нейронов скрытых слоев:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f_{11} = f(\\omega_{11}^{1} \\cdot x_1 + \\omega_{21}^{1} \\cdot x_2) \\\\\n",
    "f_{12} = f(\\omega_{12}^{1} \\cdot x_1 + \\omega_{22}^{1} \\cdot x_2) & f_{21} = f(\\omega_{11}^{2} \\cdot f_{11} + \\omega_{21}^{2} \\cdot f_{12} + \\omega_{31}^{2} \\cdot f_{13}) \\\\\n",
    "f_{13} = f(\\omega_{13}^{1} \\cdot x_1 + \\omega_{23}^{1} \\cdot x_2) & f_{22} = f(\\omega_{12}^{2} \\cdot f_{11} + \\omega_{22}^{2} \\cdot f_{12} + \\omega_{32}^{2} \\cdot f_{13}) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "и последнее выходное значение $y$:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "v_{out} = \\omega_{11}^{3} \\cdot f_{21} + \\omega_{21}^{3} \\cdot f_{22} \\\\\n",
    "y = f(v_{out})\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Далее, мы знаем требуемый отклик $d$ для текущего вектора $[x_1, x_2]$, значит для него можно вычислить ошибку работы НС. Она будет равна:\n",
    "$$e = y - d$$\n",
    "\n",
    "Далее начинается самое главное – корректировка весов. Для этого делается обратный проход по НС: от последнего слоя – к первому.  \n",
    "\n",
    "Итак, у нас есть ошибка $e$ и некая функция активации нейронов $f(x)$. Первое, что нам нужно – это вычислить локальный градиент для выходного нейрона. Это делается по формуле:\n",
    "$$\\delta = e \\cdot f'(v_{out})$$\n",
    "\n",
    "Этот момент требует пояснения. Ранее используемая пороговая функция:\n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "    1, & x \\geq 0.5\\\\\n",
    "    0, & x < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "нам уже не подходит, т.к. она не дифференцируема на всем диапазоне значений $x$. Вместо этого для сетей с небольшим числом слоев, часто применяют или `гиперболический тангенс`:\n",
    "$$\n",
    "f(x)= \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "или `логистическую функцию`:\n",
    "\n",
    "$$\n",
    "f(x)= \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Например, выберем логистическую функцию.\n",
    "\n",
    "Ее производная функции по аргументу $x$ дает очень простое выражение:\n",
    "\n",
    "$$\n",
    "f'(x)= f(x) \\cdot (1 - f(x))\n",
    "$$\n",
    "\n",
    "Именно его мы и запишем в нашу формулу вычисления локального градиента:\n",
    "$$\\delta = e \\cdot f'(v_{out}) = e \\cdot f(v_{out}) \\cdot (1 - f(v_{out})$$\n",
    "\n",
    "Но, так как\n",
    "$$y = f(v_{out})$$\n",
    "\n",
    "то локальный градиент последнего нейрона, равен:\n",
    "$$\\delta = e \\cdot y \\cdot (1 - y)$$\n",
    "\n",
    "Теперь у нас есть все, чтобы выполнить коррекцию весов. Начнем со связи $\\omega_{11}^{3}$, формула будет такой:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{3} = \\omega_{11}^{3} - \\lambda \\cdot \\delta \\cdot f_{21} \\\\\n",
    "\\omega_{21}^{3} = \\omega_{11}^{3} - \\lambda \\cdot \\delta \\cdot f_{22}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Здесь у вас может возникнуть вопрос: что такое параметр $\\lambda$ и где его брать? Он подбирается самостоятельно, вручную самим разработчиком. В самом простом случае можно попробовать следующие значения:\n",
    "$$\\lambda = 0.1, 0.01, 0.001, \\dots$$\n",
    "\n",
    "Итак, мы с вами скорректировали связи последнего слоя. Если вам все это понятно, значит, вы уже практически поняли весь алгоритм обучения, потому что дальше действуем подобным образом. Переходим к нейрону следующего с конца слоя и для его входящих связей повторим ту же самую процедуру. Но для этого нужно знать значение его локального градиента. Определяется он просто. Локальный градиент последнего нейрона взвешивается весами входящих в него связей. Полученные значения на каждом нейроне умножаются на производную функции активации, взятую в точках входной суммы:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\delta_{21} = \\delta \\cdot \\omega_{11}^{3} \\cdot f'(v_{21}) = \\delta \\cdot \\omega_{11}^{3} \\cdot [f_{21} \\cdot (1 - f_{21})] \\\\\n",
    "\\delta_{22} = \\delta \\cdot \\omega_{21}^{3} \\cdot f'(v_{22}) = \\delta \\cdot \\omega_{21}^{3} \\cdot [f_{22} \\cdot (1 - f_{22})]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "А дальше действуем по такой же самой схеме, корректируем входные связи по той же формуле:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{2} = \\omega_{11}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{11} \\\\\n",
    "\\omega_{12}^{2} = \\omega_{12}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{12} \\\\\n",
    "\\omega_{13}^{2} = \\omega_{13}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{13}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "И для второго нейрона:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{21}^{2} = \\omega_{21}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{11} \\\\\n",
    "\\omega_{22}^{2} = \\omega_{22}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{12} \\\\\n",
    "\\omega_{23}^{2} = \\omega_{23}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{13}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Осталось скорректировать веса первого слоя. Снова вычисляем локальные градиенты для нейронов первого слоя, но так как каждый из них имеет два выхода, то сначала вычисляем сумму от каждого выхода:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\sigma_{11} = \\delta_{21} \\cdot \\omega_{11}^{2} + \\delta_{22} \\cdot \\omega_{21}^{2} \\\\\n",
    "\\sigma_{12} = \\delta_{21} \\cdot \\omega_{12}^{2} + \\delta_{22} \\cdot \\omega_{22}^{2} \\\\\n",
    "\\sigma_{13} = \\delta_{21} \\cdot \\omega_{13}^{2} + \\delta_{22} \\cdot \\omega_{23}^{2}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "А затем, значения локальных градиентов на нейронах первого скрытого слоя:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\delta_{11} = \\sigma_{11} \\cdot f'(v_{11}) = \\sigma_{11} \\cdot [f_{11} \\cdot (1 - f_{11})] \\\\\n",
    "\\delta_{12} = \\sigma_{12} \\cdot f'(v_{12}) = \\sigma_{12} \\cdot [f_{12} \\cdot (1 - f_{12})] \\\\\n",
    "\\delta_{13} = \\sigma_{13} \\cdot f'(v_{13}) = \\sigma_{13} \\cdot [f_{13} \\cdot (1 - f_{13})]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Ну и осталось выполнить коррекцию весов первого слоя все по той же формуле:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{1} = \\omega_{11}^{1} - \\lambda \\cdot \\delta_{11} \\cdot x_1 \\\\\n",
    "\\omega_{12}^{1} = \\omega_{12}^{1} - \\lambda \\cdot \\delta_{11} \\cdot x_2 \\\\\n",
    "\\omega_{21}^{1} = \\omega_{21}^{1} - \\lambda \\cdot \\delta_{12} \\cdot x_1 \\\\\n",
    "\\omega_{22}^{1} = \\omega_{22}^{1} - \\lambda \\cdot \\delta_{12} \\cdot x_2 \\\\\n",
    "\\omega_{31}^{1} = \\omega_{31}^{1} - \\lambda \\cdot \\delta_{13} \\cdot x_1 \\\\\n",
    "\\omega_{32}^{1} = \\omega_{32}^{1} - \\lambda \\cdot \\delta_{13} \\cdot x_2\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "В результате, мы выполнили одну итерацию алгоритма обучения НС. На следующей итерации мы должны взять другой входной вектор из нашего обучающего множества. Лучше всего это сделать случайным образом, чтобы не формировались возможные ложные закономерности в последовательности данных при обучении НС. Повторяя много раз этот процесс, весовые связи будут все точнее описывать обучающую выборку.\n",
    "\n",
    "Процесс обучения в целом мы рассмотрели. Но какой критерий качества минимизировался алгоритмом градиентного спуска? В действительности, мы стремились получить минимум суммы квадратов ошибок для обучающей выборки:\n",
    "$$\n",
    "E = \\frac{1}{2} \\cdot \\sum_{j=1}^{N} e_j^2 = \\frac{1}{2} \\cdot \\sum_{j=1}^{N} (d_j -y_j)^2\n",
    "$$\n",
    "\n",
    "То есть, с помощью алгоритма градиентного спуска веса корректируются так, чтобы минимизировать этот критерий качества работы НС."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ebbcf-37ea-47e2-857d-034bf1443330",
   "metadata": {},
   "source": [
    "### 7.5 Датасет `Rain in Australia`\n",
    "\n",
    "Датасет содержит данные о метеонаблюдениях в Австралии, цель - прогнозирование дождя на следующий день. Целевой признак - `RainTomorrow`.\n",
    "\n",
    "| Column        | Meaning                                                                                                | Units               |\n",
    "|---------------|--------------------------------------------------------------------------------------------------------|---------------------|\n",
    "| Location      | The common name of the location of the weather station                                                 |                     |\n",
    "| MinTemp       | Minimum temperature in the 24 hours to 9am. Sometimes only known to the nearest whole degree.          | degrees Celsius     |\n",
    "| MaxTemp       | Maximum temperature in the 24 hours from 9am. Sometimes only known to the nearest whole degree.        | degrees Celsius     |\n",
    "| Rainfall      | Precipitation (rainfall) in the 24 hours to 9am. Sometimes only known to the nearest whole millimetre. | millimetres         |\n",
    "| Sunshine      | Bright sunshine in the 24 hours to midnight                                                            | hours               |\n",
    "| WindGustDir   | Direction of strongest gust in the 24 hours to midnight                                                | 16 compass points   |\n",
    "| WindGustSpeed | Speed of strongest wind gust in the 24 hours to midnight                                               | kilometres per hour |\n",
    "| WindDir9am    | Wind direction averaged over 10 minutes prior to 9 am                                                  | compass points      |\n",
    "| WindDir3pm    | Wind direction averaged over 10 minutes prior to 3 pm                                                  | compass points      |\n",
    "| WindSpeed9am  | Wind speed averaged over 10 minutes prior to 9 am                                                      | kilometres per hour |\n",
    "| WindSpeed3pm  | Wind speed averaged over 10 minutes prior to 3 pm                                                      | kilometres per hour |\n",
    "| Humidity9am   | Relative humidity at 9 am                                                                              | percent             |\n",
    "| Humidity3pm   | Relative humidity at 3 pm                                                                              | percent             |\n",
    "| Pressure9am   | Atmospheric pressure reduced to mean sea level at 9 am                                                 | hectopascals        |\n",
    "| Pressure3pm   | Atmospheric pressure reduced to mean sea level at 3 pm                                                 | hectopascals        |\n",
    "| Cloud9am      | Fraction of sky obscured by cloud at 9 am                                                              | eighths             |\n",
    "| Cloud3pm      | Fraction of sky obscured by cloud at 3 pm                                                              | eighths             |\n",
    "| Temp9am       | Temperature at 9 am                                                                                    | degrees Celsius     |\n",
    "| Temp3pm       | Temperature at 3 pm                                                                                    | degrees Celsius     |\n",
    "| RainToday     | The rain for that day was 1mm or more                                                                  | Yes or No           |\n",
    "| RainTomorrow  | The rain for that day was 1mm or more. The target variable to predict.                                 | Yes or No           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758018b-fd21-4ac1-9cca-5d27930a2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# путь к папке с данными\n",
    "data_path = \"..\\lecture_3\\data\"\n",
    "# датасет: Rain in Australia: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package\n",
    "df = pd.read_csv(Path(data_path, 'weatherAUS.csv'))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a86cc3-0dfb-47e8-9448-f2eeccac3f5d",
   "metadata": {},
   "source": [
    "### 7.6 Подготовка данных\n",
    "\n",
    "Более подробно процедуры предобработки данных описаны в лекциях 3-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c89f5-1dc0-4295-a641-e41c62d9af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Drop NaN in RainTomorrow\n",
    "df = df.drop(df[df['RainTomorrow'].isna()].index)\n",
    "# Cat cols\n",
    "cat_cols = [var for var in df.columns if df[var].dtype == 'object']\n",
    "cat_null = df[cat_cols].isnull().sum()\n",
    "cat_null_mode = df[cat_null[cat_null > 0].index].mode()\n",
    "print(f\"Cat cols with NaNs mode: {cat_null_mode}\")\n",
    "# Fill NaNs\n",
    "for col in cat_cols:\n",
    "    df.fillna({col: df[col].mode()[0]}, inplace=True)\n",
    "# Print cat cols cardinality\n",
    "len_max = max([len(col) for col in cat_cols])\n",
    "for col in cat_cols:\n",
    "    print(f\"{col:<{len_max}} labels: {len(df[col].unique())}\")\n",
    "# Split Date into Day, Month, Year\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "cat_cols.remove('Date')\n",
    "# Num cols\n",
    "num_cols = [var for var in df.columns if not df[var].dtype == 'object']\n",
    "# Fill NaNs\n",
    "for col in num_cols:\n",
    "    df.fillna({col: df[col].median()}, inplace=True)\n",
    "# Encode Location\n",
    "df_loc_dummy = pd.get_dummies(df.Location, prefix='Location')\n",
    "df = df.drop('Location', axis = 1)\n",
    "df = df.join(df_loc_dummy)\n",
    "# Cat cols left\n",
    "cat_left = [var for var in df.columns if df[var].dtype == 'object']\n",
    "cat_left = ['WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "# Encode cat cols\n",
    "df = pd.get_dummies(data=df, columns=cat_left, drop_first=False)\n",
    "# Replace values\n",
    "df.replace({'RainToday': {'No': 0, 'Yes': 1}}, inplace = True)\n",
    "df.replace({'RainTomorrow': {'No': 0, 'Yes': 1}}, inplace = True)\n",
    "\n",
    "num_cols_ext = deepcopy(num_cols)\n",
    "num_cols_ext.append('RainToday')\n",
    "# Scale data\n",
    "all_cols = list(df.columns)\n",
    "mm_scaler = MinMaxScaler()\n",
    "features_scaled = mm_scaler.fit_transform(df[all_cols])\n",
    "df_scaled = pd.DataFrame(features_scaled, columns=all_cols)\n",
    "\n",
    "# Outliers bounds\n",
    "def get_bounds(dataframe, col):\n",
    "    iqr = dataframe[col].quantile(0.75) - dataframe[col].quantile(0.25)\n",
    "    lower_bound = dataframe[col].quantile(0.25) - 1.5 * iqr\n",
    "    upper_bound = dataframe[col].quantile(0.75) + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "num_cols_clean = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
    "\n",
    "bounds_dict = dict()\n",
    "\n",
    "for col in num_cols_clean:\n",
    "    lb, ub = get_bounds(df_scaled, col)\n",
    "    bounds_dict[col] = [lb, ub]\n",
    "    print(f\"{col:<13} outliers are values < {lb:.2f} or > {ub:.2f}\")\n",
    "\n",
    "# Clean outliers\n",
    "def clean_data(df, bounds_dict: dict):\n",
    "    df_clean = deepcopy(df)\n",
    "    print(df_clean.shape)\n",
    "\n",
    "    for k, v in bounds_dict.items():\n",
    "        arr = np.array((df_clean[k] > v[0]) & (df_clean[k] < v[1])).reshape((-1, 1))\n",
    "        print(f\"{k}: bounds: {v}\")\n",
    "        print(f\"  old: {df_clean[k].shape[0]}, new: {np.count_nonzero(arr)}, diff: {np.count_nonzero(arr) - df_clean[k].shape[0]}\")\n",
    "        df_clean = df_clean[(df_clean[k] > v[0]) & (df_clean[k] < v[1])]\n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_data(df_scaled, bounds_dict)\n",
    "\n",
    "X = df_clean.drop(['RainTomorrow'], axis=1)\n",
    "y = df_clean['RainTomorrow']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a4f2d-78e5-4676-b697-de0c7d329cd5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 7.7 Классификация\n",
    "#### Перцептрон\n",
    "\n",
    " - [Perceptron @ sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    " - [MLPClassifier @ sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ba440-dc31-4e8a-9100-4d1fbb3f8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "from ipywidgets import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a84772-a1f7-4ecb-98c9-14ec55956248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "p_model = Perceptron(tol=1e-4, random_state=444)\n",
    "p_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064b04f-a862-4953-9c46-e9ab1d837e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = p_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, digits=7)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d56802-62c1-4e59-b3ad-7b8019675fe3",
   "metadata": {},
   "source": [
    "#### Многослойный перцептрон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2532cae-8f58-40da-a4cd-35b78d0e311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(tol=1e-4, max_iter=300, random_state=444)\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50f64c-7f61-404b-a267-109886369ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, digits=7)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d534a5-72e6-4521-bb2e-3b210f8f9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "loss_values = mlp_model.loss_curve_\n",
    "\n",
    "ax.plot(loss_values)\n",
    "ax.set_title('Кривая потерь')\n",
    "ax.set_xlabel('Номер итерации')\n",
    "ax.set_ylabel('Ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5f2ba-8454-4c03-8390-82a7968dfc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(tol=1e-4, max_iter=300, hidden_layer_sizes=(200,), random_state=444)\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed6f16-148c-4118-9f27-c4a1981ba777",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, digits=7)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855fce7-11fe-4c26-b325-1bbd50d838ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "loss_values = mlp_model.loss_curve_\n",
    "ax.plot(loss_values)\n",
    "ax.set_title('Кривая потерь')\n",
    "ax.set_xlabel('Номер итерации')\n",
    "ax.set_ylabel('Ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff63824-9b65-4363-b566-c29b84f7ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(tol=1e-4, max_iter=300, hidden_layer_sizes=(100,), activation='logistic', random_state=444)\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90758f3e-e10a-4002-8c54-81943cd0f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, digits=7)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccf57a-8846-422e-8645-ff0badc529c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "loss_values = mlp_model.loss_curve_\n",
    "ax.plot(loss_values)\n",
    "ax.set_title('Кривая потерь')\n",
    "ax.set_xlabel('Номер итерации')\n",
    "ax.set_ylabel('Ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50176df3-93e8-481d-b904-0b1082e4955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(tol=1e-4, max_iter=300, hidden_layer_sizes=(100,), activation='logistic', early_stopping=True, random_state=444)\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d80a6-1d5d-4756-8f57-833c8629bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, digits=7)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de709f48-f24a-41b8-bff4-70d87d5d5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "loss_values = mlp_model.loss_curve_\n",
    "val_scores = mlp_model.validation_scores_\n",
    "ax.plot(loss_values)\n",
    "ax.plot(val_scores)\n",
    "ax.set_title('Кривая потерь')\n",
    "ax.set_xlabel('Номер итерации')\n",
    "ax.set_ylabel('Ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c168f5-e7be-4764-ac7e-de5c52ac1604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
